{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy import env\n",
    "import os\n",
    "import glob\n",
    "from arcgis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "from arcgis.features import GeoSeriesAccessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# pd.DataFrame.spatial.from_featureclass(???)  \n",
    "# df.spatial.to_featureclass(location=???,sanitize_columns=False)  \n",
    "\n",
    "# gsa = arcgis.features.GeoSeriesAccessor(df['SHAPE'])  \n",
    "# df['AREA'] = gsa.area  # KNOW YOUR UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values in Spatially enabled dataframes (ignores SHAPE column)\n",
    "def fill_na_sedf(df_with_shape_column, fill_value=0):\n",
    "    if 'SHAPE' in list(df_with_shape_column.columns):\n",
    "        df = df_with_shape_column.copy()\n",
    "        shape_column = df['SHAPE'].copy()\n",
    "        del df['SHAPE']\n",
    "        return df.fillna(fill_value).merge(shape_column,left_index=True, right_index=True, how='inner')\n",
    "    else:\n",
    "        raise Exception(\"Dataframe does not include 'SHAPE' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Outputs'):\n",
    "    os.makedirs('Outputs')\n",
    "    \n",
    "outputs = ['.\\\\Outputs', \"REMM_Forecast_Review_TAZ.gdb\", \"REMM_Forecast_Review_DISTMED.gdb\", 'Scratch.gdb']\n",
    "gdb = os.path.join(outputs[0], outputs[1])\n",
    "gdb2 = os.path.join(outputs[0], outputs[2])\n",
    "scratch = os.path.join(outputs[0], outputs[3])\n",
    "\n",
    "if not arcpy.Exists(gdb):\n",
    "    arcpy.CreateFileGDB_management(outputs[0], outputs[1])\n",
    "\n",
    "if not arcpy.Exists(gdb2):\n",
    "    arcpy.CreateFileGDB_management(outputs[0], outputs[2])\n",
    "\n",
    "if not arcpy.Exists(scratch):\n",
    "    arcpy.CreateFileGDB_management(outputs[0], outputs[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_shp = pd.DataFrame.spatial.from_featureclass(r\".\\Ancillary\\Boundaries.gdb\\taz_utm12\")\n",
    "distmed_shp = pd.DataFrame.spatial.from_featureclass(r\".\\Ancillary\\Boundaries.gdb\\distmed_utm12\")\n",
    "taz_se = glob.glob(os.path.join(r'.\\SE_Data','SE_*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = taz_se[0]\n",
    "# year = os.path.splitext(os.path.basename(csv))[0].split('_')[-1]\n",
    "\n",
    "# # read in csv and format\n",
    "# df = pd.read_csv(taz_se[0])\n",
    "# df.rename({';TAZID':'TAZID'}, inplace=True, axis=1)\n",
    "# df = df[['TAZID', 'TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']].copy()\n",
    "\n",
    "# # rename columns\n",
    "# data_cols = ['TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']\n",
    "# new_data_cols = [f'{col}_{year[2:4]}' for col in data_cols]\n",
    "# myDict = { k:v for (k,v) in zip(data_cols, new_data_cols)}\n",
    "# df.rename(myDict, inplace=True, axis=1)\n",
    "\n",
    "# # new columns/ratios ???\n",
    "# df[f'HHJOBI_{year[2:4]}'] = (df[f'TOTHH_{year[2:4]}'] * 1.8 ) + df[f'TOTEMP_{year[2:4]}']\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = taz_shp[taz_shp['REMM']==1].copy()\n",
    "base_cols = ['TAZID', 'ACRES', 'DEVACRES', 'CO_NAME', 'DISTMED', 'CityArea', 'OtherArea', 'SHAPE']\n",
    "data_cols = ['TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR', 'HHJOBI', 'RET_JOBS_RATIO','RET_HH_RATIO','HH_JOBS_RATIO']\n",
    "base = base[base_cols].copy()\n",
    "\n",
    "\n",
    "for csv in taz_se:\n",
    "\n",
    "    year = os.path.splitext(os.path.basename(csv))[0].split('_')[-1]\n",
    "    # read in csv and format\n",
    "    df = pd.read_csv(csv)\n",
    "    df.rename({';TAZID':'TAZID'}, inplace=True, axis=1)\n",
    "    df = df[['TAZID', 'TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']].copy()\n",
    "\n",
    "    # new columns/ratios ???\n",
    "    df['HHJOBI'] = (df['TOTHH'] * 1.8 ) + df['TOTEMP']\n",
    "    df['RET_JOBS_RATIO'] = (df['RETEMP']) / (df['TOTEMP'])\n",
    "    df['RET_HH_RATIO'] = (df['RETEMP']) / (df['TOTHH'])\n",
    "    df['HH_JOBS_RATIO'] = (df['TOTHH']) / (df['TOTEMP'])\n",
    "    df['HHJOBI'] = df['HHJOBI'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    df['RET_JOBS_RATIO'] = df['RET_JOBS_RATIO'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    df['RET_HH_RATIO'] = df['RET_HH_RATIO'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    df['HH_JOBS_RATIO'] = df['HH_JOBS_RATIO'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "    # rename columns\n",
    "    new_data_cols = [f'{col}_{year[2:4]}' for col in data_cols]\n",
    "    myDict = { k:v for (k,v) in zip(data_cols, new_data_cols)}\n",
    "    df.rename(myDict, inplace=True, axis=1)\n",
    "\n",
    "    base = base.merge(df,on='TAZID', how='left')\n",
    "\n",
    "# store all column names\n",
    "all_columns = list(base.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_for_distmed_sum = ('TOTHH', 'HHPOP', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR')\n",
    "data_cols_for_distmed_sum = [x for x in list(base.columns) if x.startswith(data_cols_for_distmed_sum)]\n",
    "base_distmed = base.groupby('DISTMED')[data_cols_for_distmed_sum].sum().reset_index()\n",
    "\n",
    "data_cols_for_distmed_avg = ('HHSIZE')\n",
    "data_cols_for_distmed_avg = [x for x in list(base.columns) if x.startswith(data_cols_for_distmed_avg)]\n",
    "base_distmed_avg = base.groupby('DISTMED')[data_cols_for_distmed_avg].mean().reset_index()\n",
    "\n",
    "base_distmed = base_distmed.merge(base_distmed_avg, on='DISTMED', how='left')\n",
    "\n",
    "for y in range(19,51):\n",
    "    base_distmed[f'HHJOBI_{y}'] = (base_distmed[f'TOTHH_{y}'] * 1.8 ) + base_distmed[f'TOTEMP_{y}']\n",
    "    base_distmed[f'RET_JOBS_RATIO_{y}'] = (base_distmed[f'RETEMP_{y}']) / (base_distmed[f'TOTEMP_{y}'])\n",
    "    base_distmed[f'RET_HH_RATIO_{y}'] = (base_distmed[f'RETEMP_{y}']) / (base_distmed[f'TOTHH_{y}'])\n",
    "    base_distmed[f'HH_JOBS_RATIO_{y}'] = (base_distmed[f'TOTHH_{y}']) / (base_distmed[f'TOTEMP_{y}'])\n",
    "    base_distmed[f'HHJOBI_{y}'] = base_distmed[f'HHJOBI_{y}'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    base_distmed[f'RET_JOBS_RATIO_{y}'] = base_distmed[f'RET_JOBS_RATIO_{y}'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    base_distmed[f'RET_HH_RATIO_{y}'] = base_distmed[f'RET_HH_RATIO_{y}'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "    base_distmed[f'HH_JOBS_RATIO_{y}'] = base_distmed[f'HH_JOBS_RATIO_{y}'].fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "base_distmed = distmed_shp[['DISTMED','ACRES', 'DEVACRES', 'SHAPE']].merge(base_distmed, on='DISTMED', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Decades\n",
    "decade_cols = [col for col in all_columns if col.endswith(('_20','_30','_40','_50'))] # using tuple\n",
    "# decade_years = [col for col in all_columns if any(yr in col for yr in ['_20','_30','_40','_50'])] # using \"any\" keyword\n",
    "decade_cols.sort()\n",
    "decade_export = base[base_cols + decade_cols].copy()\n",
    "\n",
    "base_cols_distmed = ['DISTMED', 'SHAPE']\n",
    "decade_export_by_distmed = base_distmed[base_cols_distmed + decade_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\REMM_Forecast_Review_DISTMED.gdb\\\\SE_2020_2030_2040_2050_By_DISTMED'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "decade_export.spatial.to_featureclass(location=os.path.join(gdb, 'SE_2020_2030_2040_2050_By_TAZ'),sanitize_columns=False)\n",
    "decade_export_by_distmed.spatial.to_featureclass(location=os.path.join(gdb2, 'SE_2020_2030_2040_2050_By_DISTMED'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on TOTHH...\n",
      "working on HHPOP...\n",
      "working on HHSIZE...\n",
      "working on TOTEMP...\n",
      "working on RETEMP...\n",
      "working on INDEMP...\n",
      "working on OTHEMP...\n",
      "working on ALLEMP...\n",
      "working on RETL...\n",
      "working on FOOD...\n",
      "working on MANU...\n",
      "working on WSLE...\n",
      "working on OFFI...\n",
      "working on GVED...\n",
      "working on HLTH...\n",
      "working on OTHR...\n",
      "working on HHJOBI...\n",
      "working on RET_JOBS_RATIO...\n",
      "working on RET_HH_RATIO...\n",
      "working on HH_JOBS_RATIO...\n"
     ]
    }
   ],
   "source": [
    "# SE Categories\n",
    "for col in data_cols:\n",
    "    print(f'working on {col}...')\n",
    "    se_cols = [c for c in all_columns if c.startswith(col)]\n",
    "    se_cols.sort()\n",
    "    taz_se_export = base[base_cols + se_cols].copy()\n",
    "    distmed_se_export = base_distmed[base_cols_distmed + se_cols].copy()\n",
    "\n",
    "    taz_se_export.spatial.to_featureclass(location=os.path.join(gdb, f'SE_{col}_All_Years_By_TAZ'),sanitize_columns=False)\n",
    "    distmed_se_export.spatial.to_featureclass(location=os.path.join(gdb2, f'SE_{col}_All_Years_By_DISTMED'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2050 Policy Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Columns (12) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "zb = pd.read_csv(r\".\\Inputs\\zoning_baseline_20221006.csv\")\n",
    "zpp = pd.read_csv(r\".\\Inputs\\zoning_parcels_p_20221006.csv\")\n",
    "p = pd.read_csv(r\".\\Inputs\\parcels_20221006.csv\")\n",
    "b = pd.DataFrame.spatial.from_featureclass(r\"E:\\Projects\\REMM-Manage-Base-Year-Data\\Current_Inputs\\remm_base_year.gdb\\buildings\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format zpp\n",
    "zpp_most_recent = zpp.sort_values('year', ascending=False).drop_duplicates('parcel_id').sort_index()\n",
    "zpp_most_recent['year'] = 2019\n",
    "# zpp_most_recent.to_csv(os.path.join(outputs[0], 'zpp_most_recent.csv'), index=False)\n",
    "zpp_most_recent_ids = zpp_most_recent['parcel_id'].to_list()\n",
    "del zpp_most_recent['TAZID_900']\n",
    "del zpp_most_recent['locnote']\n",
    "del zpp_most_recent['AreaName']\n",
    "del zpp_most_recent['mponote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format zb, get most recent year\n",
    "lu = {'t':1, 'f':0}\n",
    "\n",
    "zb['type1'] = zb['type1'].map(lu)\n",
    "zb['type2'] = zb['type2'].map(lu)\n",
    "zb['type3'] = zb['type3'].map(lu)\n",
    "zb['type4'] = zb['type4'].map(lu)\n",
    "zb['type5'] = zb['type5'].map(lu)\n",
    "zb['type6'] = zb['type6'].map(lu)\n",
    "zb['type7'] = zb['type7'].map(lu)\n",
    "zb['type8'] = zb['type8'].map(lu)\n",
    "\n",
    "zb = zb[zb['parcel_id'].isin(zpp_most_recent_ids) == False].copy()\n",
    "\n",
    "del zb['CO_NAME']\n",
    "del zb['max_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712236, 14)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine zoning and join with the parcels\n",
    "zb_2050 = pd.concat([zb,zpp_most_recent])\n",
    "# zb_2050.to_csv(os.path.join(outputs[0], 'zb_2050.csv'), index=False)\n",
    "p = p[['parcel_id', 'TAZID_900', 'parcel_acres']].copy()\n",
    "pzb_2050 = p.merge(zb_2050, on='parcel_id', how='inner')\n",
    "\n",
    "pzb_2050['max_dua'].fillna(0, inplace=True)\n",
    "pzb_2050['max_far'].fillna(0, inplace=True)\n",
    "\n",
    "pzb_2050.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity Summary by TAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vacant parcels\n",
    "b_vacant = b[b['building_type_id2'].isin([0.0,15.0])==True].copy()\n",
    "b_vacant = b_vacant[['parcel_id', 'building_type_id2']].copy()\n",
    "p_vacant = p.merge(b, on='parcel_id', how='inner')\n",
    "\n",
    "# summarize total vacant acres\n",
    "vacant_acres_by_taz = p_vacant[p_vacant['building_type_id2']<1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "vacant_acres_by_taz.columns = ['TAZID_900','vacant_acres']\n",
    "\n",
    "# summarize total vacant + farm acres\n",
    "vacant_farm_acres_by_taz = p_vacant.groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "vacant_farm_acres_by_taz.columns = ['TAZID_900','vacant_and_farm_acres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     499730\n",
       "0.0      76577\n",
       "2.0      26529\n",
       "99.0     13998\n",
       "15.0     13335\n",
       "4.0      11359\n",
       "3.0      11272\n",
       "14.0      8549\n",
       "12.0      8187\n",
       "6.0       7383\n",
       "5.0       7014\n",
       "10.0      2442\n",
       "9.0       1563\n",
       "8.0       1139\n",
       "13.0       800\n",
       "11.0       257\n",
       "16.0       226\n",
       "7.0         31\n",
       "Name: building_type_id2, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['building_type_id2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pzb_2050['du'] = pzb_2050['parcel_acres'] * pzb_2050['max_dua']\n",
    "pzb_2050['fa'] = pzb_2050['parcel_acres'] * pzb_2050['max_far']\n",
    "\n",
    "pzb_2050['du'] = pzb_2050['du'].round(2)\n",
    "pzb_2050['fa'] = pzb_2050['fa'].round(2)\n",
    "\n",
    "capacity_by_taz  = pzb_2050.groupby('TAZID_900')[['du', 'fa']].sum().reset_index()\n",
    "\n",
    "mean_capacity_by_taz = pzb_2050.groupby('TAZID_900')[['max_dua', 'max_far']].mean().reset_index()\n",
    "mean_capacity_by_taz.columns = ['TAZID_900','mean_max_dua', 'mean_max_far']\n",
    "median_capacity_by_taz = pzb_2050.groupby('TAZID_900')[['max_dua', 'max_far']].median().reset_index()\n",
    "median_capacity_by_taz.columns = ['TAZID_900','med_max_dua', 'med_max_far']\n",
    "\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID', 'DISTMED', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)\n",
    "\n",
    "capacity_by_taz = (base.merge(capacity_by_taz, on='TAZID_900', how='left')\n",
    "                       .merge(vacant_acres_by_taz, on='TAZID_900', how='left')\n",
    "                       .merge(vacant_farm_acres_by_taz, on='TAZID_900', how='left')\n",
    "                       .merge(mean_capacity_by_taz, on='TAZID_900', how='left')\n",
    "                       .merge(median_capacity_by_taz, on='TAZID_900', how='left'))\n",
    "\n",
    "\n",
    "# aggregate to Medium District\n",
    "capacity_by_distmed_sum = capacity_by_taz.groupby('DISTMED')[['du', 'fa', 'vacant_acres','vacant_and_farm_acres']].sum().reset_index()\n",
    "capacity_by_distmed_avg = capacity_by_taz.groupby('DISTMED')[['mean_max_dua','mean_max_far']].mean().reset_index()\n",
    "capacity_by_distmed_med = capacity_by_taz.groupby('DISTMED')[['med_max_dua','med_max_far']].median().reset_index()\n",
    "capacity_by_distmed = (distmed_shp[['DISTMED', 'ACRES', 'DEVACRES', 'SHAPE']].merge(capacity_by_distmed_sum, on='DISTMED', how='left')\n",
    "                                                        .merge(capacity_by_distmed_avg, on='DISTMED', how='left')\n",
    "                                                        .merge(capacity_by_distmed_med, on='DISTMED', how='left'))\n",
    "\n",
    "\n",
    "capacity_by_taz['dua_taz'] = capacity_by_taz['du'] / capacity_by_taz['DEVACRES']\n",
    "capacity_by_taz['far_taz'] = capacity_by_taz['fa'] / capacity_by_taz['DEVACRES']\n",
    "capacity_by_taz['dua_taz'] = capacity_by_taz['du'].round(2)\n",
    "capacity_by_taz['far_taz'] = capacity_by_taz['fa'].round(2)\n",
    "\n",
    "capacity_by_distmed['dua_distmed'] = capacity_by_distmed['du'] / capacity_by_distmed['DEVACRES']\n",
    "capacity_by_distmed['far_distmed'] = capacity_by_distmed['fa'] / capacity_by_distmed['DEVACRES']\n",
    "capacity_by_distmed['dua_distmed'] = capacity_by_distmed['du'].round(2)\n",
    "capacity_by_distmed['far_distmed'] = capacity_by_distmed['fa'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\REMM_Forecast_Review_DISTMED.gdb\\\\Capacity_Summary_By_DISTMED'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "capacity_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Capacity_Summary_By_TAZ'),sanitize_columns=False)\n",
    "capacity_by_distmed.spatial.to_featureclass(location=os.path.join(gdb2, 'Capacity_Summary_By_DISTMED'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zoning types summary by taz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single family\n",
    "acres_type1_by_taz = pzb_2050[pzb_2050['type1']==1].groupby('TAZID_900', )[['parcel_acres']].sum().reset_index()\n",
    "acres_type1_by_taz.columns = ['TAZID_900','sf_acres']\n",
    "\n",
    "# multifamily\n",
    "acres_type2_by_taz = pzb_2050[pzb_2050['type2']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type2_by_taz.columns = ['TAZID_900','mf_acres']\n",
    "\n",
    "# residential\n",
    "acres_res_by_taz = pzb_2050[(pzb_2050['type1']==1) | (pzb_2050['type2']==1)].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_res_by_taz.columns = ['TAZID_900','res_acres']\n",
    "\n",
    "# industrial\n",
    "acres_type3_by_taz = pzb_2050[pzb_2050['type3']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type3_by_taz.columns = ['TAZID_900','ind_acres']\n",
    "\n",
    "# retail\n",
    "acres_type4_by_taz = pzb_2050[pzb_2050['type4']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type4_by_taz.columns = ['TAZID_900','ret_acres']\n",
    "\n",
    "# office\n",
    "acres_type5_by_taz = pzb_2050[pzb_2050['type5']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type5_by_taz.columns = ['TAZID_900','off_acres']\n",
    "\n",
    "# commercial\n",
    "acres_com_by_taz = pzb_2050[(pzb_2050['type3']==1) | (pzb_2050['type4']==1) | (pzb_2050['type5']==1)].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_com_by_taz.columns = ['TAZID_900','com_acres']\n",
    "acres_com_by_taz['com_acres'].fillna(0, inplace=True)\n",
    "\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID','DISTMED', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)\n",
    "acres_type_by_taz = (base.merge(acres_type1_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type2_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type3_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type4_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type5_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_res_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_com_by_taz, on='TAZID_900', how='left'))\n",
    "\n",
    "# fill NAs\n",
    "acres_type_by_taz = fill_na_sedf(acres_type_by_taz, 0)\n",
    "\n",
    "# aggregate to Medium District\n",
    "acres_type_by_distmed = acres_type_by_taz.groupby('DISTMED')[['sf_acres', 'mf_acres', 'res_acres', 'ind_acres', 'ret_acres', 'off_acres','com_acres']].sum().reset_index()\n",
    "acres_type_by_distmed = distmed_shp[['DISTMED','ACRES', 'DEVACRES', 'SHAPE']].merge(acres_type_by_distmed, on='DISTMED', how='left') \n",
    "\n",
    "# calulate proportions get acreage from TAZ\n",
    "acres_type_by_taz['res_acres_proportion'] = acres_type_by_taz['res_acres'] / acres_type_by_taz['DEVACRES']\n",
    "acres_type_by_taz['com_acres_proportion'] = acres_type_by_taz['com_acres'] / acres_type_by_taz['DEVACRES']\n",
    "acres_type_by_distmed['res_acres_proportion'] = acres_type_by_distmed['res_acres'] / acres_type_by_distmed['DEVACRES']\n",
    "acres_type_by_distmed['com_acres_proportion'] = acres_type_by_distmed['com_acres'] / acres_type_by_distmed['DEVACRES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\REMM_Forecast_Review_DISTMED.gdb\\\\Buildable_Types_Summary_By_DISTMED'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "acres_type_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Buildable_Types_Summary_By_TAZ'),sanitize_columns=False)\n",
    "acres_type_by_distmed.spatial.to_featureclass(location=os.path.join(gdb2, 'Buildable_Types_Summary_By_DISTMED'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Units by decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv(r\".\\Inputs\\buildings_20221006.csv\")\n",
    "b_2030 = pd.read_csv(r\".\\Inputs\\run773year2030allbuildings.csv\")\n",
    "b_2040 = pd.read_csv(r\".\\Inputs\\run773year2040allbuildings.csv\")\n",
    "b_2050 = pd.read_csv(r\".\\Inputs\\run773year2050allbuildings.csv\")\n",
    "\n",
    "# calculate the decade\n",
    "def add_built_decade(df):\n",
    "\n",
    "    df['BUILT_DECADE'] = 'NA'\n",
    "    df.loc[(df['year_built'] >= 1840) & (df['year_built'] < 1850), 'BUILT_DECADE'] = \"1840's\"\n",
    "    df.loc[(df['year_built'] >= 1850) & (df['year_built'] < 1860), 'BUILT_DECADE'] = \"1850's\"\n",
    "    df.loc[(df['year_built'] >= 1860) & (df['year_built'] < 1870), 'BUILT_DECADE'] = \"1860's\"\n",
    "    df.loc[(df['year_built'] >= 1870) & (df['year_built'] < 1880), 'BUILT_DECADE'] = \"1870's\"\n",
    "    df.loc[(df['year_built'] >= 1880) & (df['year_built'] < 1890), 'BUILT_DECADE'] = \"1880's\"\n",
    "    df.loc[(df['year_built'] >= 1890) & (df['year_built'] < 1900), 'BUILT_DECADE'] = \"1890's\"\n",
    "    df.loc[(df['year_built'] >= 1900) & (df['year_built'] < 1910), 'BUILT_DECADE'] = \"1900's\"\n",
    "    df.loc[(df['year_built'] >= 1910) & (df['year_built'] < 1920), 'BUILT_DECADE'] = \"1910's\"\n",
    "    df.loc[(df['year_built'] >= 1920) & (df['year_built'] < 1930), 'BUILT_DECADE'] = \"1920's\"\n",
    "    df.loc[(df['year_built'] >= 1930) & (df['year_built'] < 1940), 'BUILT_DECADE'] = \"1930's\"\n",
    "    df.loc[(df['year_built'] >= 1940) & (df['year_built'] < 1950), 'BUILT_DECADE'] = \"1940's\"\n",
    "    df.loc[(df['year_built'] >= 1950) & (df['year_built'] < 1960), 'BUILT_DECADE'] = \"1950's\"\n",
    "    df.loc[(df['year_built'] >= 1960) & (df['year_built'] < 1970), 'BUILT_DECADE'] = \"1960's\"\n",
    "    df.loc[(df['year_built'] >= 1970) & (df['year_built'] < 1980), 'BUILT_DECADE'] = \"1970's\"\n",
    "    df.loc[(df['year_built'] >= 1980) & (df['year_built'] < 1990), 'BUILT_DECADE'] = \"1980's\"\n",
    "    df.loc[(df['year_built'] >= 1990) & (df['year_built'] < 2000), 'BUILT_DECADE'] = \"1990's\"\n",
    "    df.loc[(df['year_built'] >= 2000) & (df['year_built'] < 2010), 'BUILT_DECADE'] = \"2000's\"\n",
    "    df.loc[(df['year_built'] >= 2010) & (df['year_built'] < 2020), 'BUILT_DECADE'] = \"2010's\"\n",
    "    df.loc[(df['year_built'] >= 2020) & (df['year_built'] < 2030), 'BUILT_DECADE'] = \"2020's\"\n",
    "    df.loc[(df['year_built'] >= 2030) & (df['year_built'] < 2040), 'BUILT_DECADE'] = \"2030's\"\n",
    "    df.loc[(df['year_built'] >= 2040) & (df['year_built'] < 2050), 'BUILT_DECADE'] = \"2040's\"\n",
    "    df.loc[(df['year_built'] >= 2050) & (df['year_built'] < 2060), 'BUILT_DECADE'] = \"2050's\"\n",
    "\n",
    "    return df\n",
    "\n",
    "b = add_built_decade(b) \n",
    "b_2030 = add_built_decade(b_2030) \n",
    "b_2040 = add_built_decade(b_2040) \n",
    "b_2050 = add_built_decade(b_2050)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_taz = p[['parcel_id', 'TAZID_900']].copy()\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID','DISTMED', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_res = b[b['building_type_id'].isin([1,2])==True].copy()\n",
    "b_res = b_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "b_2030_res = b_2030[b_2030['building_type_id'].isin([1,2])==True].copy()\n",
    "b_2030_res = b_2030_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "b_2040_res = b_2040[b_2040['building_type_id'].isin([1,2])==True].copy()\n",
    "b_2040_res = b_2040_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "b_2050_res = b_2050[b_2050['building_type_id'].isin([1,2])==True].copy()\n",
    "b_2050_res = b_2050_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "# 1990's\n",
    "sf_units_built_90s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"1990's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_90s_by_taz.columns = ['TAZID_900','sf_units_built_1990s']\n",
    "\n",
    "mf_units_built_90s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"1990's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_90s_by_taz.columns = ['TAZID_900','mf_units_built_1990s']\n",
    "\n",
    "res_units_built_90s_by_taz = b_res[(b_res['BUILT_DECADE']==\"1990's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_90s_by_taz.columns = ['TAZID_900','res_units_built_1990s']\n",
    "\n",
    "# 2000's\n",
    "sf_units_built_00s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"2000's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_00s_by_taz.columns = ['TAZID_900','sf_units_built_2000s']\n",
    "\n",
    "mf_units_built_00s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"2000's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_00s_by_taz.columns = ['TAZID_900','mf_units_built_2000s']\n",
    "\n",
    "res_units_built_00s_by_taz = b_res[(b_res['BUILT_DECADE']==\"2000's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_00s_by_taz.columns = ['TAZID_900','res_units_built_2000s']\n",
    "\n",
    "# 2010's\n",
    "sf_units_built_10s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"2010's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_10s_by_taz.columns = ['TAZID_900','sf_units_built_2010s']\n",
    "\n",
    "mf_units_built_10s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"2010's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_10s_by_taz.columns = ['TAZID_900','mf_units_built_2010s']\n",
    "\n",
    "res_units_built_10s_by_taz = b_res[(b_res['BUILT_DECADE']==\"2010's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_10s_by_taz.columns = ['TAZID_900','res_units_built_2010s']\n",
    "\n",
    "# 2020's\n",
    "sf_units_built_20s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_20s_by_taz.columns = ['TAZID_900','sf_units_built_2020s']\n",
    "\n",
    "mf_units_built_20s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_20s_by_taz.columns = ['TAZID_900','mf_units_built_2020s']\n",
    "\n",
    "res_units_built_20s_by_taz = b_res[(b_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_20s_by_taz.columns = ['TAZID_900','res_units_built_2020s']\n",
    "\n",
    "# REMM 2020's\n",
    "remm_sf_units_built_20s_by_taz = b_2030_res[(b_2030_res['building_type_id']==1) & (b_2030_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_sf_units_built_20s_by_taz.columns = ['TAZID_900','REMM_sf_units_built_2020s']\n",
    "\n",
    "remm_mf_units_built_20s_by_taz = b_2030_res[(b_2030_res['building_type_id']==2) & (b_2030_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_mf_units_built_20s_by_taz.columns = ['TAZID_900','REMM_mf_units_built_2020s']\n",
    "\n",
    "remm_res_units_built_20s_by_taz = b_2030_res[(b_2030_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_res_units_built_20s_by_taz.columns = ['TAZID_900','REMM_res_units_built_2020s']\n",
    "\n",
    "# REMM 2030's\n",
    "remm_sf_units_built_30s_by_taz = b_2040_res[(b_2040_res['building_type_id']==1) & (b_2040_res['BUILT_DECADE']==\"2030's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_sf_units_built_30s_by_taz.columns = ['TAZID_900','REMM_sf_units_built_2030s']\n",
    "\n",
    "remm_mf_units_built_30s_by_taz = b_2040_res[(b_2040_res['building_type_id']==2) & (b_2040_res['BUILT_DECADE']==\"2030's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_mf_units_built_30s_by_taz.columns = ['TAZID_900','REMM_mf_units_built_2030s']\n",
    "\n",
    "remm_res_units_built_30s_by_taz = b_2040_res[(b_2040_res['BUILT_DECADE']==\"2030's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_res_units_built_30s_by_taz.columns = ['TAZID_900','REMM_res_units_built_2030s']\n",
    "\n",
    "# REMM 2040's\n",
    "remm_sf_units_built_40s_by_taz = b_2050_res[(b_2050_res['building_type_id']==1) & (b_2050_res['BUILT_DECADE']==\"2040's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_sf_units_built_40s_by_taz.columns = ['TAZID_900','REMM_sf_units_built_2040s']\n",
    "\n",
    "remm_mf_units_built_40s_by_taz = b_2050_res[(b_2050_res['building_type_id']==2) & (b_2050_res['BUILT_DECADE']==\"2040's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_mf_units_built_40s_by_taz.columns = ['TAZID_900','REMM_mf_units_built_2040s']\n",
    "\n",
    "remm_res_units_built_40s_by_taz = b_2050_res[(b_2050_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_res_units_built_40s_by_taz.columns = ['TAZID_900','REMM_res_units_built_2040s']\n",
    "\n",
    "\n",
    "res_units_built_by_taz = (base.merge(sf_units_built_90s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_90s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_90s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(sf_units_built_00s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_00s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_00s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(sf_units_built_10s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_10s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_10s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(sf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_sf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_mf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_res_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_sf_units_built_30s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_mf_units_built_30s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_res_units_built_30s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_sf_units_built_40s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_mf_units_built_40s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_res_units_built_40s_by_taz, on='TAZID_900', how='left'))\n",
    "\n",
    "# fill NAs\n",
    "res_units_built_by_taz = fill_na_sedf(res_units_built_by_taz, 0)\n",
    "\n",
    "# aggregate to Medium District\n",
    "res_units_built_by_distmed = res_units_built_by_taz.groupby('DISTMED')[['sf_units_built_1990s', 'mf_units_built_1990s', 'res_units_built_1990s', \n",
    "                                                                        'sf_units_built_2000s', 'mf_units_built_2000s', 'res_units_built_2000s',\n",
    "                                                                        'sf_units_built_2010s', 'mf_units_built_2010s', 'res_units_built_2010s',\n",
    "                                                                        'sf_units_built_2020s', 'mf_units_built_2020s', 'res_units_built_2020s',\n",
    "                                                                        'REMM_sf_units_built_2020s', 'REMM_mf_units_built_2020s', 'REMM_res_units_built_2020s',\n",
    "                                                                        'REMM_sf_units_built_2030s', 'REMM_mf_units_built_2030s', 'REMM_res_units_built_2030s',\n",
    "                                                                        'REMM_sf_units_built_2040s', 'REMM_mf_units_built_2040s', 'REMM_res_units_built_2040s']].sum().reset_index()\n",
    "res_units_built_by_distmed = distmed_shp[['DISTMED','ACRES', 'DEVACRES', 'SHAPE']].merge(res_units_built_by_distmed, on='DISTMED', how='left') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\REMM_Forecast_Review_DISTMED.gdb\\\\Residential_Units_Built_By_DISTMED'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "res_units_built_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Residential_Units_Built_By_TAZ'),sanitize_columns=False)\n",
    "res_units_built_by_distmed.spatial.to_featureclass(location=os.path.join(gdb2, 'Residential_Units_Built_By_DISTMED'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_2019 = pd.read_csv(r\".\\Inputs\\trips_generated_by_taz_2019_20221006.csv\")\n",
    "trips_2019.columns = ['TAZID_900', 'prod_2019', 'attr_2019', 'total_2019']\n",
    "trips_2050 = pd.read_csv(r\".\\Inputs\\trips_generated_by_taz_2050_20221006.csv\")\n",
    "trips_2050.columns = ['TAZID_900', 'prod_2050', 'attr_2050', 'total_2050']\n",
    "\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID','DISTMED', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)\n",
    "\n",
    "trips_by_taz = (base.merge(trips_2019, on='TAZID_900', how='left')\n",
    "                              .merge(trips_2050, on='TAZID_900', how='left'))\n",
    "\n",
    "# fill NAs\n",
    "trips_by_taz = fill_na_sedf(trips_by_taz, 0)\n",
    "\n",
    "# aggregate to Medium District\n",
    "trips_by_distmed = trips_by_taz.groupby('DISTMED')[['prod_2019', 'attr_2019', 'total_2019', \n",
    "                                                    'prod_2050', 'attr_2050', 'total_2050']].sum().reset_index()\n",
    "trips_by_distmed = distmed_shp[['DISTMED','ACRES', 'DEVACRES', 'SHAPE']].merge(trips_by_distmed, on='DISTMED', how='left') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\REMM_Forecast_Review_DISTMED.gdb\\\\Trips_By_DISTMED'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "trips_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Trips_By_TAZ'),sanitize_columns=False)\n",
    "trips_by_distmed.spatial.to_featureclass(location=os.path.join(gdb2, 'Trips_By_DISTMED'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
