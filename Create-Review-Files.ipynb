{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "from arcpy import env\n",
    "import os\n",
    "import glob\n",
    "from arcgis import GIS\n",
    "from arcgis.features import GeoAccessor\n",
    "from arcgis.features import GeoSeriesAccessor\n",
    "import pandas as pd\n",
    "\n",
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.parallelProcessingFactor = \"90%\"\n",
    "\n",
    "# show all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# pd.DataFrame.spatial.from_featureclass(???)  \n",
    "# df.spatial.to_featureclass(location=???,sanitize_columns=False)  \n",
    "\n",
    "# gsa = arcgis.features.GeoSeriesAccessor(df['SHAPE'])  \n",
    "# df['AREA'] = gsa.area  # KNOW YOUR UNITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA values in Spatially enabled dataframes (ignores SHAPE column)\n",
    "def fill_na_sedf(df_with_shape_column, fill_value=0):\n",
    "    if 'SHAPE' in list(df_with_shape_column.columns):\n",
    "        df = df_with_shape_column.copy()\n",
    "        shape_column = df['SHAPE'].copy()\n",
    "        del df['SHAPE']\n",
    "        return df.fillna(fill_value).merge(shape_column,left_index=True, right_index=True, how='inner')\n",
    "    else:\n",
    "        raise Exception(\"Dataframe does not include 'SHAPE' column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Outputs'):\n",
    "    os.makedirs('Outputs')\n",
    "    \n",
    "outputs = ['.\\\\Outputs', \"REMM_Forecast_Review.gdb\"]\n",
    "gdb = os.path.join(outputs[0], outputs[1])\n",
    "if not arcpy.Exists(gdb):\n",
    "    arcpy.CreateFileGDB_management(outputs[0], outputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "taz_shp = pd.DataFrame.spatial.from_featureclass(r\".\\Ancillary\\TAZ_900.gdb\\taz_utm12\")\n",
    "taz_se = glob.glob(os.path.join(r'.\\SE_Data','SE_*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv = taz_se[0]\n",
    "# year = os.path.splitext(os.path.basename(csv))[0].split('_')[-1]\n",
    "\n",
    "# # read in csv and format\n",
    "# df = pd.read_csv(taz_se[0])\n",
    "# df.rename({';TAZID':'TAZID'}, inplace=True, axis=1)\n",
    "# df = df[['TAZID', 'TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']].copy()\n",
    "\n",
    "# # rename columns\n",
    "# data_cols = ['TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']\n",
    "# new_data_cols = [f'{col}_{year[2:4]}' for col in data_cols]\n",
    "# myDict = { k:v for (k,v) in zip(data_cols, new_data_cols)}\n",
    "# df.rename(myDict, inplace=True, axis=1)\n",
    "\n",
    "# # new columns/ratios ???\n",
    "# df[f'HHJOBI_{year[2:4]}'] = (df[f'TOTHH_{year[2:4]}'] * 1.8 ) + df[f'TOTEMP_{year[2:4]}']\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = taz_shp[taz_shp['REMM']==1].copy()\n",
    "base_cols = ['TAZID', 'ACRES', 'DEVACRES', 'CO_NAME', 'DISTMED', 'CityArea', 'OtherArea', 'SHAPE']\n",
    "data_cols = ['TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR', 'HHJOBI']\n",
    "base = base[base_cols].copy()\n",
    "\n",
    "\n",
    "for csv in taz_se:\n",
    "\n",
    "    year = os.path.splitext(os.path.basename(csv))[0].split('_')[-1]\n",
    "    # read in csv and format\n",
    "    df = pd.read_csv(csv)\n",
    "    df.rename({';TAZID':'TAZID'}, inplace=True, axis=1)\n",
    "    df = df[['TAZID', 'TOTHH', 'HHPOP', 'HHSIZE', 'TOTEMP', 'RETEMP', 'INDEMP', 'OTHEMP', 'ALLEMP', 'RETL', 'FOOD', 'MANU', 'WSLE', 'OFFI', 'GVED', 'HLTH', 'OTHR']].copy()\n",
    "\n",
    "    # new columns/ratios ???\n",
    "    df['HHJOBI'] = (df['TOTHH'] * 1.8 ) + df['TOTEMP']\n",
    "\n",
    "    # rename columns\n",
    "    new_data_cols = [f'{col}_{year[2:4]}' for col in data_cols]\n",
    "    myDict = { k:v for (k,v) in zip(data_cols, new_data_cols)}\n",
    "    df.rename(myDict, inplace=True, axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    base = base.merge(df,on='TAZID', how='left')\n",
    "\n",
    "# store all column names\n",
    "all_columns = list(base.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\scratch.gdb\\\\se_2020_2030_2040_2050'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Decades\n",
    "decade_cols = [col for col in all_columns if col.endswith(('_20','_30','_40','_50'))] # using tuple\n",
    "# decade_years = [col for col in all_columns if any(yr in col for yr in ['_20','_30','_40','_50'])] # using \"any\" keyword\n",
    "decade_cols.sort()\n",
    "decade_export = base[base_cols + decade_cols].copy()\n",
    "decade_export.spatial.to_featureclass(location=os.path.join(gdb, 'se_2020_2030_2040_2050'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on TOTHH...\n",
      "working on HHPOP...\n",
      "working on HHSIZE...\n",
      "working on TOTEMP...\n",
      "working on RETEMP...\n",
      "working on INDEMP...\n",
      "working on OTHEMP...\n",
      "working on ALLEMP...\n",
      "working on RETL...\n",
      "working on FOOD...\n",
      "working on MANU...\n",
      "working on WSLE...\n",
      "working on OFFI...\n",
      "working on GVED...\n",
      "working on HLTH...\n",
      "working on OTHR...\n"
     ]
    }
   ],
   "source": [
    "# SE Categories\n",
    "for col in data_cols:\n",
    "    print(f'working on {col}...')\n",
    "    se_cols = [c for c in all_columns if c.startswith(col)]\n",
    "    se_cols.sort()\n",
    "    se_export = base[base_cols + se_cols].copy()\n",
    "    se_export.spatial.to_featureclass(location=os.path.join(gdb, f'SE_{col}_All_Years'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\scratch.gdb\\\\SE_2020_2030_2040_2050'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decade_export.spatial.to_featureclass(location=os.path.join(gdb, 'SE_2020_2030_2040_2050'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2050 Policy Output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "zb = pd.read_csv(r\".\\Inputs\\zoning_baseline_20221006.csv\")\n",
    "zpp = pd.read_csv(r\".\\Inputs\\zoning_parcels_p_20221006.csv\")\n",
    "p = pd.read_csv(r\".\\Inputs\\parcels_20221006.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format zpp\n",
    "zpp_most_recent = zpp.sort_values('year', ascending=False).drop_duplicates('parcel_id').sort_index()\n",
    "zpp_most_recent['year'] = 2019\n",
    "# zpp_most_recent.to_csv(os.path.join(outputs[0], 'zpp_most_recent.csv'), index=False)\n",
    "zpp_most_recent_ids = zpp_most_recent['parcel_id'].to_list()\n",
    "del zpp_most_recent['TAZID_900']\n",
    "del zpp_most_recent['locnote']\n",
    "del zpp_most_recent['AreaName']\n",
    "del zpp_most_recent['mponote']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format zb, get most recent year\n",
    "lu = {'t':1, 'f':0}\n",
    "\n",
    "zb['type1'] = zb['type1'].map(lu)\n",
    "zb['type2'] = zb['type2'].map(lu)\n",
    "zb['type3'] = zb['type3'].map(lu)\n",
    "zb['type4'] = zb['type4'].map(lu)\n",
    "zb['type5'] = zb['type5'].map(lu)\n",
    "zb['type6'] = zb['type6'].map(lu)\n",
    "zb['type7'] = zb['type7'].map(lu)\n",
    "zb['type8'] = zb['type8'].map(lu)\n",
    "\n",
    "zb = zb[zb['parcel_id'].isin(zpp_most_recent_ids) == False].copy()\n",
    "\n",
    "del zb['CO_NAME']\n",
    "del zb['max_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712236, 14)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine zoning and join with the parcels\n",
    "zb_2050 = pd.concat([zb,zpp_most_recent])\n",
    "# zb_2050.to_csv(os.path.join(outputs[0], 'zb_2050.csv'), index=False)\n",
    "p = p[['parcel_id', 'TAZID_900', 'parcel_acres']].copy()\n",
    "pzb_2050 = p.merge(zb_2050, on='parcel_id', how='inner')\n",
    "\n",
    "pzb_2050['max_dua'].fillna(0, inplace=True)\n",
    "pzb_2050['max_far'].fillna(0, inplace=True)\n",
    "\n",
    "pzb_2050.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacity Summary by TAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\scratch.gdb\\\\Capacity_Summary_By_TAZ'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pzb_2050['du'] = pzb_2050['parcel_acres'] * pzb_2050['max_dua']\n",
    "pzb_2050['fa'] = pzb_2050['parcel_acres'] * pzb_2050['max_far']\n",
    "\n",
    "pzb_2050['du'] = pzb_2050['du'].round(2)\n",
    "pzb_2050['fa'] = pzb_2050['fa'].round(2)\n",
    "\n",
    "capacity_by_taz  = pzb_2050.groupby('TAZID_900')[['du', 'fa']].sum().reset_index()\n",
    "\n",
    "mean_capacity_by_taz = pzb_2050.groupby('TAZID_900')[['max_dua', 'max_far']].mean().reset_index()\n",
    "mean_capacity_by_taz.columns = ['TAZID_900','mean_max_dua', 'mean_max_far']\n",
    "median_capacity_by_taz = pzb_2050.groupby('TAZID_900')[['max_dua', 'max_far']].median().reset_index()\n",
    "median_capacity_by_taz.columns = ['TAZID_900','med_max_dua', 'med_max_far']\n",
    "\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)\n",
    "\n",
    "capacity_by_taz = (base.merge(capacity_by_taz, on='TAZID_900', how='left')\n",
    "                       .merge(mean_capacity_by_taz, on='TAZID_900', how='left')\n",
    "                       .merge(median_capacity_by_taz, on='TAZID_900', how='left'))\n",
    "\n",
    "capacity_by_taz['dua_taz'] = capacity_by_taz['du'] / capacity_by_taz['DEVACRES']\n",
    "capacity_by_taz['far_taz'] = capacity_by_taz['fa'] / capacity_by_taz['DEVACRES']\n",
    "\n",
    "pzb_2050['dua_taz'] = pzb_2050['du'].round(2)\n",
    "pzb_2050['far_taz'] = pzb_2050['fa'].round(2)\n",
    "\n",
    "capacity_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Capacity_Summary_By_TAZ'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zoning types summary by taz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\scratch.gdb\\\\Buildable_Types_Summary_By_TAZ'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single family\n",
    "acres_type1_by_taz = pzb_2050[pzb_2050['type1']==1].groupby('TAZID_900', )[['parcel_acres']].sum().reset_index()\n",
    "acres_type1_by_taz.columns = ['TAZID_900','sf_acres']\n",
    "\n",
    "# multifamily\n",
    "acres_type2_by_taz = pzb_2050[pzb_2050['type2']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type2_by_taz.columns = ['TAZID_900','mf_acres']\n",
    "\n",
    "# residential\n",
    "acres_res_by_taz = pzb_2050[(pzb_2050['type1']==1) | (pzb_2050['type2']==1)].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_res_by_taz.columns = ['TAZID_900','res_acres']\n",
    "\n",
    "# industrial\n",
    "acres_type3_by_taz = pzb_2050[pzb_2050['type3']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type3_by_taz.columns = ['TAZID_900','ind_acres']\n",
    "\n",
    "# retail\n",
    "acres_type4_by_taz = pzb_2050[pzb_2050['type4']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type4_by_taz.columns = ['TAZID_900','ret_acres']\n",
    "\n",
    "# office\n",
    "acres_type5_by_taz = pzb_2050[pzb_2050['type5']==1].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_type5_by_taz.columns = ['TAZID_900','off_acres']\n",
    "\n",
    "# commercial\n",
    "acres_com_by_taz = pzb_2050[(pzb_2050['type3']==1) | (pzb_2050['type4']==1) | (pzb_2050['type5']==1)].groupby('TAZID_900')[['parcel_acres']].sum().reset_index()\n",
    "acres_com_by_taz.columns = ['TAZID_900','com_acres']\n",
    "acres_com_by_taz['com_acres'].fillna(0, inplace=True)\n",
    "\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)\n",
    "acres_type_by_taz = (base.merge(acres_type1_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type2_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type3_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type4_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_type5_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_res_by_taz, on='TAZID_900', how='left')\n",
    "                         .merge(acres_com_by_taz, on='TAZID_900', how='left'))\n",
    "\n",
    "# fill NAs\n",
    "acres_type_by_taz = fill_na_sedf(acres_type_by_taz, 0)\n",
    "\n",
    "# calulate proportions get acreage from TAZ\n",
    "acres_type_by_taz['res_acres_proportion'] = acres_type_by_taz['res_acres'] / acres_type_by_taz['DEVACRES']\n",
    "acres_type_by_taz['com_acres_proportion'] = acres_type_by_taz['com_acres'] / acres_type_by_taz['DEVACRES']\n",
    "\n",
    "acres_type_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Buildable_Types_Summary_By_TAZ'),sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Units by decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv(r\".\\Inputs\\buildings_20221006.csv\")\n",
    "b_2030 = pd.read_csv(r\".\\Inputs\\run773year2030allbuildings.csv\")\n",
    "b_2040 = pd.read_csv(r\".\\Inputs\\run773year2040allbuildings.csv\")\n",
    "b_2050 = pd.read_csv(r\".\\Inputs\\run773year2050allbuildings.csv\")\n",
    "\n",
    "# calculate the decade\n",
    "def add_built_decade(df):\n",
    "\n",
    "    df['BUILT_DECADE'] = 'NA'\n",
    "    df.loc[(df['year_built'] >= 1840) & (df['year_built'] < 1850), 'BUILT_DECADE'] = \"1840's\"\n",
    "    df.loc[(df['year_built'] >= 1850) & (df['year_built'] < 1860), 'BUILT_DECADE'] = \"1850's\"\n",
    "    df.loc[(df['year_built'] >= 1860) & (df['year_built'] < 1870), 'BUILT_DECADE'] = \"1860's\"\n",
    "    df.loc[(df['year_built'] >= 1870) & (df['year_built'] < 1880), 'BUILT_DECADE'] = \"1870's\"\n",
    "    df.loc[(df['year_built'] >= 1880) & (df['year_built'] < 1890), 'BUILT_DECADE'] = \"1880's\"\n",
    "    df.loc[(df['year_built'] >= 1890) & (df['year_built'] < 1900), 'BUILT_DECADE'] = \"1890's\"\n",
    "    df.loc[(df['year_built'] >= 1900) & (df['year_built'] < 1910), 'BUILT_DECADE'] = \"1900's\"\n",
    "    df.loc[(df['year_built'] >= 1910) & (df['year_built'] < 1920), 'BUILT_DECADE'] = \"1910's\"\n",
    "    df.loc[(df['year_built'] >= 1920) & (df['year_built'] < 1930), 'BUILT_DECADE'] = \"1920's\"\n",
    "    df.loc[(df['year_built'] >= 1930) & (df['year_built'] < 1940), 'BUILT_DECADE'] = \"1930's\"\n",
    "    df.loc[(df['year_built'] >= 1940) & (df['year_built'] < 1950), 'BUILT_DECADE'] = \"1940's\"\n",
    "    df.loc[(df['year_built'] >= 1950) & (df['year_built'] < 1960), 'BUILT_DECADE'] = \"1950's\"\n",
    "    df.loc[(df['year_built'] >= 1960) & (df['year_built'] < 1970), 'BUILT_DECADE'] = \"1960's\"\n",
    "    df.loc[(df['year_built'] >= 1970) & (df['year_built'] < 1980), 'BUILT_DECADE'] = \"1970's\"\n",
    "    df.loc[(df['year_built'] >= 1980) & (df['year_built'] < 1990), 'BUILT_DECADE'] = \"1980's\"\n",
    "    df.loc[(df['year_built'] >= 1990) & (df['year_built'] < 2000), 'BUILT_DECADE'] = \"1990's\"\n",
    "    df.loc[(df['year_built'] >= 2000) & (df['year_built'] < 2010), 'BUILT_DECADE'] = \"2000's\"\n",
    "    df.loc[(df['year_built'] >= 2010) & (df['year_built'] < 2020), 'BUILT_DECADE'] = \"2010's\"\n",
    "    df.loc[(df['year_built'] >= 2020) & (df['year_built'] < 2030), 'BUILT_DECADE'] = \"2020's\"\n",
    "    df.loc[(df['year_built'] >= 2030) & (df['year_built'] < 2040), 'BUILT_DECADE'] = \"2030's\"\n",
    "    df.loc[(df['year_built'] >= 2040) & (df['year_built'] < 2050), 'BUILT_DECADE'] = \"2040's\"\n",
    "    df.loc[(df['year_built'] >= 2050) & (df['year_built'] < 2060), 'BUILT_DECADE'] = \"2050's\"\n",
    "\n",
    "    return df\n",
    "\n",
    "b = add_built_decade(b) \n",
    "b_2030 = add_built_decade(b_2030) \n",
    "b_2040 = add_built_decade(b_2040) \n",
    "b_2050 = add_built_decade(b_2050)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_taz = p[['parcel_id', 'TAZID_900']].copy()\n",
    "base = taz_shp[taz_shp['REMM']==1][['SA_TAZID', 'ACRES','DEVACRES','SHAPE']].copy()\n",
    "base.rename({'SA_TAZID':'TAZID_900'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Projects\\\\REMM-TAZ-SE-Review-For-MB\\\\Outputs\\\\scratch.gdb\\\\Residential_Units_Built_By_TAZ'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_res = b[b['building_type_id'].isin([1,2])==True].copy()\n",
    "b_res = b_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "b_2030_res = b_2030[b_2030['building_type_id'].isin([1,2])==True].copy()\n",
    "b_2030_res = b_2030_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "b_2040_res = b_2040[b_2040['building_type_id'].isin([1,2])==True].copy()\n",
    "b_2040_res = b_2040_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "b_2050_res = b_2050[b_2050['building_type_id'].isin([1,2])==True].copy()\n",
    "b_2050_res = b_2050_res.merge(p_taz, on='parcel_id', how='left')\n",
    "\n",
    "# 1990's\n",
    "sf_units_built_90s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"1990's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_90s_by_taz.columns = ['TAZID_900','sf_units_built_1990s']\n",
    "\n",
    "mf_units_built_90s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"1990's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_90s_by_taz.columns = ['TAZID_900','mf_units_built_1990s']\n",
    "\n",
    "res_units_built_90s_by_taz = b_res[(b_res['BUILT_DECADE']==\"1990's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_90s_by_taz.columns = ['TAZID_900','res_units_built_1990s']\n",
    "\n",
    "# 2000's\n",
    "sf_units_built_00s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"2000's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_00s_by_taz.columns = ['TAZID_900','sf_units_built_2000s']\n",
    "\n",
    "mf_units_built_00s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"2000's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_00s_by_taz.columns = ['TAZID_900','mf_units_built_2000s']\n",
    "\n",
    "res_units_built_00s_by_taz = b_res[(b_res['BUILT_DECADE']==\"2000's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_00s_by_taz.columns = ['TAZID_900','res_units_built_2000s']\n",
    "\n",
    "# 2010's\n",
    "sf_units_built_10s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"2010's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_10s_by_taz.columns = ['TAZID_900','sf_units_built_2010s']\n",
    "\n",
    "mf_units_built_10s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"2010's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_10s_by_taz.columns = ['TAZID_900','mf_units_built_2010s']\n",
    "\n",
    "res_units_built_10s_by_taz = b_res[(b_res['BUILT_DECADE']==\"2010's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_10s_by_taz.columns = ['TAZID_900','res_units_built_2010s']\n",
    "\n",
    "# 2020's\n",
    "sf_units_built_20s_by_taz = b_res[(b_res['building_type_id']==1) & (b_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "sf_units_built_20s_by_taz.columns = ['TAZID_900','sf_units_built_2020s']\n",
    "\n",
    "mf_units_built_20s_by_taz = b_res[(b_res['building_type_id']==2) & (b_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "mf_units_built_20s_by_taz.columns = ['TAZID_900','mf_units_built_2020s']\n",
    "\n",
    "res_units_built_20s_by_taz = b_res[(b_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "res_units_built_20s_by_taz.columns = ['TAZID_900','res_units_built_2020s']\n",
    "\n",
    "# REMM 2020's\n",
    "remm_sf_units_built_20s_by_taz = b_2030_res[(b_2030_res['building_type_id']==1) & (b_2030_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_sf_units_built_20s_by_taz.columns = ['TAZID_900','REMM_sf_units_built_2020s']\n",
    "\n",
    "remm_mf_units_built_20s_by_taz = b_2030_res[(b_2030_res['building_type_id']==2) & (b_2030_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_mf_units_built_20s_by_taz.columns = ['TAZID_900','REMM_mf_units_built_2020s']\n",
    "\n",
    "remm_res_units_built_20s_by_taz = b_2030_res[(b_2030_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_res_units_built_20s_by_taz.columns = ['TAZID_900','REMM_res_units_built_2020s']\n",
    "\n",
    "# REMM 2030's\n",
    "remm_sf_units_built_30s_by_taz = b_2040_res[(b_2040_res['building_type_id']==1) & (b_2040_res['BUILT_DECADE']==\"2030's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_sf_units_built_30s_by_taz.columns = ['TAZID_900','REMM_sf_units_built_2030s']\n",
    "\n",
    "remm_mf_units_built_30s_by_taz = b_2040_res[(b_2040_res['building_type_id']==2) & (b_2040_res['BUILT_DECADE']==\"2030's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_mf_units_built_30s_by_taz.columns = ['TAZID_900','REMM_mf_units_built_2030s']\n",
    "\n",
    "remm_res_units_built_30s_by_taz = b_2040_res[(b_2040_res['BUILT_DECADE']==\"2030's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_res_units_built_30s_by_taz.columns = ['TAZID_900','REMM_res_units_built_2030s']\n",
    "\n",
    "# REMM 2040's\n",
    "remm_sf_units_built_40s_by_taz = b_2050_res[(b_2050_res['building_type_id']==1) & (b_2050_res['BUILT_DECADE']==\"2040's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_sf_units_built_40s_by_taz.columns = ['TAZID_900','REMM_sf_units_built_2040s']\n",
    "\n",
    "remm_mf_units_built_40s_by_taz = b_2050_res[(b_2050_res['building_type_id']==2) & (b_2050_res['BUILT_DECADE']==\"2040's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_mf_units_built_40s_by_taz.columns = ['TAZID_900','REMM_mf_units_built_2040s']\n",
    "\n",
    "remm_res_units_built_40s_by_taz = b_2050_res[(b_2050_res['BUILT_DECADE']==\"2020's\")].groupby('TAZID_900')[['residential_units']].sum().reset_index()\n",
    "remm_res_units_built_40s_by_taz.columns = ['TAZID_900','REMM_res_units_built_2040s']\n",
    "\n",
    "\n",
    "res_units_built_by_taz = (base.merge(sf_units_built_90s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_90s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_90s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(sf_units_built_00s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_00s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_00s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(sf_units_built_10s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_10s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_10s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(sf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(mf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(res_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_sf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_mf_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_res_units_built_20s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_sf_units_built_30s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_mf_units_built_30s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_res_units_built_30s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_sf_units_built_40s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_mf_units_built_40s_by_taz, on='TAZID_900', how='left')\n",
    "                              .merge(remm_res_units_built_40s_by_taz, on='TAZID_900', how='left'))\n",
    "\n",
    "# fill NAs\n",
    "res_units_built_by_taz = fill_na_sedf(res_units_built_by_taz, 0)\n",
    "\n",
    "res_units_built_by_taz.spatial.to_featureclass(location=os.path.join(gdb, 'Residential_Units_Built_By_TAZ'),sanitize_columns=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3245673af07dcc28bdd829afb187282e9288a1f8195a5928b70ecba6e5973721"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
